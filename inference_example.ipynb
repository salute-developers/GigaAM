{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me66GFwT0ABG"
      },
      "source": [
        "### Installing reqs and downloading examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlM9MI70iTIp"
      },
      "outputs": [],
      "source": [
        "# If package is not installed\n",
        "! pip install git+https://github.com/salute-developers/GigaAM.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSE4HSfr1P0B"
      },
      "outputs": [],
      "source": [
        "# Downloading wavs for examples\n",
        "!wget https://cdn.chatwm.opensmodel.sberdevices.ru/GigaAM/example.wav\n",
        "!wget https://cdn.chatwm.opensmodel.sberdevices.ru/GigaAM/long_example.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIvec0280O64"
      },
      "source": [
        "## Speech Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCgbSPkViZpF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "from typing import Dict\n",
        "\n",
        "import gigaam\n",
        "\n",
        "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
        "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "R2aUkZbG8fJ6",
        "outputId": "3172d43c-19cf-4b97-df5e-108e40d92144"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 888M/888M [02:27<00:00, 6.29MiB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ничьих не требуя похвал счастлив уж я надеждой сладкой что дева с трепетом любви посмотрит может быть украдкой на песни грешные мои у лукоморья дуб зеленый'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = gigaam.load_model(\n",
        "    \"ctc\",  # GigaAM-V2 CTC model\n",
        "    fp16_encoder=True,  # to use fp16 encoder weights - GPU only\n",
        "    use_flash=False,  # disable flash attention - colab does not support it\n",
        ")\n",
        "model.transcribe(\"example.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "5nPc8flc1U3d",
        "outputId": "cce9f875-62ad-4b48-a6a9-29701eb343ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 892M/892M [02:34<00:00, 6.07MiB/s]\n",
            "WARNING:root:flash_attn is not supported on CPU. Disabling it...\n",
            "WARNING:root:fp16 is not supported on CPU. Leaving fp32 weights...\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ничьих не требуя похвал счастлив уж я надеждой сладкой что дева с трепетом любви посмотрит может быть украдкой на песни грешные мои у лукоморья дуб зеленый'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = gigaam.load_model(\n",
        "    \"rnnt\",  # GigaAM-V2 RNNT model\n",
        "    device=\"cpu\",  # CPU-inference\n",
        ")\n",
        "model.transcribe(\"example.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78WAYQTa14Qs"
      },
      "source": [
        "### Long-Form Speech Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvQdrpqG39kQ"
      },
      "outputs": [],
      "source": [
        "!pip install gigaam[longform]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riG4tjcH8fJ7"
      },
      "source": [
        "* For long-form inference:\n",
        "  * generate [Hugging Face API token](https://huggingface.co/docs/hub/security-tokens)\n",
        "  * accept the conditions to access [pyannote/voice-activity-detection](https://huggingface.co/pyannote/voice-activity-detection) files and content\n",
        "  * accept the conditions to access [pyannote/segmentation](https://huggingface.co/pyannote/segmentation) files and content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlpX1XOX4vGw"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HF_TOKEN\"] = \"<HF_TOKEN>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI_tb_N918FS",
        "outputId": "db002337-fdd4-4fba-95e8-802e010d5303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[00:00:00 - 00:16:83]: вечерня отошла давно но в кельях тихо и темно уже и сам игумен строгий свои молитвы прекратил и кости ветхие склонил перекрестясь на одр убогий кругом и сон и тишина но церкви дверь отворена\n",
            "[00:17:10 - 00:32:61]: трепещет луч лампады и тускло озаряет он и темную живопись икон и позлощенные оклады и раздается в тишине то тяжкий вздох то шепот важный и мрачно дремлет в вышине старинный свод\n",
            "[00:32:95 - 00:49:33]: глухой и влажный стоят за клиросом чернец и грешник неподвижны оба и шепот их как глаз из гроба и грешник бледен как мертвец монах несчастный полно перестань\n",
            "[00:49:82 - 01:05:74]: ужасна исповедь злодея заплачена тобою дань тому кто в злобе пламенея лукаво грешника блюдет и к вечной гибели ведет смирись опомнись время время раскаянье покров\n",
            "[01:05:97 - 01:10:90]: я разрешу тебя грехов сложи мучительное бремя\n"
          ]
        }
      ],
      "source": [
        "model = gigaam.load_model(\"ctc\", use_flash=False)\n",
        "recognition_result = model.transcribe_longform(\"long_example.wav\")\n",
        "\n",
        "for utterance in recognition_result:\n",
        "    transcription = utterance[\"transcription\"]\n",
        "    start, end = utterance[\"boundaries\"]\n",
        "    print(f\"[{gigaam.format_time(start)} - {gigaam.format_time(end)}]: {transcription}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywEjYaAe3BMU"
      },
      "source": [
        "## Emotion recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWsJ2RuG3HNB",
        "outputId": "6d1f8abb-9fb0-4c28-c5e3-c21413a22796"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 924M/924M [02:04<00:00, 7.78MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "angry: 0.000, sad: 0.002, neutral: 0.923, positive: 0.074\n"
          ]
        }
      ],
      "source": [
        "model = gigaam.load_model(\"emo\")\n",
        "emotion2prob: Dict[str, int] = model.get_probs(\"example.wav\")\n",
        "\n",
        "print(\", \".join([f\"{emotion}: {prob:.3f}\" for emotion, prob in emotion2prob.items()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EBcRVcC3P2E"
      },
      "source": [
        "## GigaAM embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y2mEAGU3TYN",
        "outputId": "ae488719-df9f-4318-ecf7-56c95a87a4ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 887M/887M [03:21<00:00, 4.62MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.2829,  0.3638,  0.4520,  ..., -0.4743, -0.4033, -0.2417],\n",
            "         [ 0.1611, -0.5006, -0.0584,  ..., -0.6239, -0.2320, -0.2054],\n",
            "         [-1.1849, -1.0029, -0.6111,  ..., -0.5137, -0.3737, -0.2654],\n",
            "         ...,\n",
            "         [ 0.0181, -0.3763, -0.8959,  ...,  0.1716,  0.0556,  0.1298],\n",
            "         [ 0.2690, -0.0654, -0.5020,  ..., -1.4432, -1.4827, -1.4490],\n",
            "         [-1.5650, -1.6693, -1.2834,  ...,  0.5117,  0.4839,  0.0136]]],\n",
            "       device='cuda:0', grad_fn=<TransposeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# audio-only pretrained encoder\n",
        "model = gigaam.load_model(\"ssl\", use_flash=False)\n",
        "\n",
        "emb, _ = model.embed_audio(\"example.wav\")\n",
        "print(emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW8GKJrl3tm7",
        "outputId": "160f4e59-3020-4986-b35e-5c7fa6e105fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-1.0334, -0.2841, -0.3606,  ..., -0.2859, -0.6947, -0.7006],\n",
            "         [-0.4317, -0.0140, -0.9296,  ...,  0.1781,  0.2170, -0.0181],\n",
            "         [-0.9221, -1.1284, -0.6389,  ..., -1.0664, -1.3304, -1.2421],\n",
            "         ...,\n",
            "         [ 0.5749,  0.5176, -0.0996,  ...,  1.7497,  1.8691,  2.1302],\n",
            "         [-0.2919, -0.8087, -1.2554,  ..., -0.7942, -0.7634, -0.7938],\n",
            "         [-1.8086, -2.1976, -2.4012,  ...,  0.8310,  1.0165,  1.0165]]],\n",
            "       device='cuda:0', grad_fn=<TransposeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# you also can embed audio with CTC- or RNNT-finetuned encoder\n",
        "model = gigaam.load_model(\"ctc\", use_flash=False)\n",
        "\n",
        "emb, _ = model.embed_audio(\"example.wav\")\n",
        "print(emb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je4WAyLgz0Ua"
      },
      "source": [
        "## Export to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tvo_v_iz0Ua",
        "outputId": "4db517e6-7e85-4c01-c4c3-0b21744b8988"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 892M/892M [03:20<00:00, 4.66MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Succesfully ported onnx v2_rnnt_encoder to onnx/v2_rnnt_encoder.onnx.\n",
            "Succesfully ported onnx v2_rnnt_decoder to onnx/v2_rnnt_decoder.onnx.\n",
            "Succesfully ported onnx v2_rnnt_joint to onnx/v2_rnnt_joint.onnx.\n"
          ]
        }
      ],
      "source": [
        "onnx_dir = \"onnx\"\n",
        "model_type = \"rnnt\" # or \"ctc\"\n",
        "\n",
        "model = gigaam.load_model(\n",
        "    model_type,\n",
        "    fp16_encoder=False,  # only fp32 tensors\n",
        "    use_flash=False,  # disable flash attention\n",
        ")\n",
        "model.to_onnx(dir_path=onnx_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL3W76cgz0Ua"
      },
      "source": [
        "### ONNX inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "j_rjvUeJz0Ua",
        "outputId": "2f8a9e71-c5aa-4fb6-89cf-d251eeb7dce3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ничьих не требуя похвал счастлив уж я надеждой сладкой что дева с трепетом любви посмотрит может быть украдкой на песни грешные мои у лукоморья дуб зеленый'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gigaam.onnx_utils import load_onnx_sessions, transcribe_sample\n",
        "\n",
        "sessions = load_onnx_sessions(onnx_dir, model_type)\n",
        "transcribe_sample(\"example.wav\", model_type, sessions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
