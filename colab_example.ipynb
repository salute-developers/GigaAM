{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-Zq_26Ma2Dx"
   },
   "source": [
    "## GigaAM from GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyTZ5vTXbgkO"
   },
   "source": [
    "### Installing reqs and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MW_PBRka4w9",
    "outputId": "84bd793d-e0dd-4b89-aa08-01d4721752ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'gigaam'...\n",
      "remote: Enumerating objects: 504, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
      "remote: Total 504 (delta 3), reused 7 (delta 2), pack-reused 492 (from 1)\u001b[K\n",
      "Receiving objects: 100% (504/504), 2.75 MiB | 5.95 MiB/s, done.\n",
      "Resolving deltas: 100% (298/298), done.\n",
      "/content/gigaam\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/salute-developers/GigaAM.git\n",
    "%cd GigaAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFpMODKsVqrt"
   },
   "outputs": [],
   "source": [
    "! pip install -e .[tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qsuHA6RY926",
    "outputId": "b26805c3-3886-4128-d3bd-210fc939d698"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /content/gigaam\n",
      "plugins: cov-7.0.0, hydra-core-1.3.2, anyio-4.11.0, typeguard-4.4.4, langsmith-0.4.42\n",
      "collected 16 items / 12 deselected / 4 selected                                \u001b[0m\n",
      "\n",
      "tests/test_loading.py::test_model_revision_partial[emo] \u001b[32mPASSED\u001b[0m\u001b[33m           [ 25%]\u001b[0m\n",
      "tests/test_loading.py::test_model_revision_partial[v2_ssl] \u001b[32mPASSED\u001b[0m\u001b[33m        [ 50%]\u001b[0m\n",
      "tests/test_loading.py::test_model_revision_partial[v3_ctc] \u001b[32mPASSED\u001b[0m\u001b[33m        [ 75%]\u001b[0m\n",
      "tests/test_loading.py::test_model_revision_partial[v3_e2e_rnnt] \u001b[32mPASSED\u001b[0m\u001b[33m   [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=========== \u001b[32m4 passed\u001b[0m, \u001b[33m\u001b[1m12 deselected\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 167.17s (0:02:47)\u001b[0m\u001b[33m ===========\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest -v tests/test_loading.py -m partial --disable-warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBkEJhSGbmGL"
   },
   "source": [
    "### Model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqL9ZolqfihI"
   },
   "source": [
    "##### Loading arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snrIrMfOfN0Z"
   },
   "outputs": [],
   "source": [
    "import gigaam\n",
    "\n",
    "model = gigaam.load_model(\n",
    "    \"v3_e2e_rnnt\",      # model version: see readme#gigaam-family at GitHub for description\n",
    "    fp16_encoder=True,  # enabled by default on CUDA, **IGNORED ON CPU (using fp32)**\n",
    "    use_flash=False,    # disabled by default, boost performance for large tensors\n",
    "    device=None,        # use cuda if available by default, can be set to `cpu` / `cuda`\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peg7n2kqfkkW"
   },
   "source": [
    "##### Inference methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8vK2SEWVq0o",
    "outputId": "7f87497e-de4e-44a1-8394-6f735c292a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeds: tensor([[[-0.2824,  0.3641,  0.4503,  ..., -0.4728, -0.4027, -0.2415],\n",
      "         [ 0.1607, -0.4995, -0.0565,  ..., -0.6242, -0.2318, -0.2053],\n",
      "         [-1.1857, -1.0032, -0.6091,  ..., -0.5142, -0.3736, -0.2652],\n",
      "         ...,\n",
      "         [ 0.0187, -0.3757, -0.8965,  ...,  0.1718,  0.0567,  0.1307],\n",
      "         [ 0.2691, -0.0672, -0.5010,  ..., -1.4433, -1.4832, -1.4515],\n",
      "         [-1.5648, -1.6692, -1.2828,  ...,  0.5110,  0.4826,  0.0133]]],\n",
      "       device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Transcription: Ничьих не требуя похвал, Счастлив уж я надеждой сладкой, Что дева с трепетом любви Посмотрит, может быть, украдкой На песни грешные мои. У лукоморья дуб зелёный.\n",
      "\n",
      "Emotions: angry: 0.000, sad: 0.002, neutral: 0.923, positive: 0.075\n"
     ]
    }
   ],
   "source": [
    "# Load test audio\n",
    "audio_path = gigaam.utils.download_short_audio()\n",
    "\n",
    "# Audio embeddings\n",
    "model_name = \"v2_ssl\"       # Options: `v1_ssl`, `v2_ssl`, `v3_ssl`\n",
    "model = gigaam.load_model(model_name)\n",
    "embedding, _ = model.embed_audio(audio_path)\n",
    "print(\"Embeds:\", embedding)\n",
    "\n",
    "# ASR\n",
    "model_name = \"v3_e2e_rnnt\"  # Options: any model version with suffix `_ctc` or `_rnnt`\n",
    "model = gigaam.load_model(model_name)\n",
    "transcription = model.transcribe(audio_path)\n",
    "print(\"\\nTranscription:\", transcription)\n",
    "\n",
    "# Emotion recognition\n",
    "model = gigaam.load_model(\"emo\")\n",
    "emotion2prob = model.get_probs(audio_path)\n",
    "print(\"\\nEmotions:\", \", \".join([f\"{emotion}: {prob:.3f}\" for emotion, prob in emotion2prob.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP3QqjPpwZYl"
   },
   "source": [
    "### ONNX convertation and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NnKh1tnjwYZw",
    "outputId": "8786532b-a4b9-4fe2-8936-a7db4e57b38d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully ported onnx v2_ssl_encoder to onnx/v2_ssl_encoder.onnx.\n",
      "Succesfully ported onnx v3_e2e_rnnt_encoder to onnx/v3_e2e_rnnt_encoder.onnx.\n",
      "Succesfully ported onnx v3_e2e_rnnt_decoder to onnx/v3_e2e_rnnt_decoder.onnx.\n",
      "Succesfully ported onnx v3_e2e_rnnt_joint to onnx/v3_e2e_rnnt_joint.onnx.\n",
      "Succesfully ported onnx emo to onnx/emo.onnx.\n"
     ]
    }
   ],
   "source": [
    "onnx_dir = \"onnx\"\n",
    "\n",
    "gigaam.load_model(\"v2_ssl\").to_onnx(dir_path=onnx_dir)\n",
    "gigaam.load_model(\"v3_e2e_rnnt\").to_onnx(dir_path=onnx_dir)\n",
    "gigaam.load_model(\"emo\").to_onnx(dir_path=onnx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uk2zWgmIwt5n",
    "outputId": "e3850b76-c20d-4f60-c49b-2ca8053b8d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeds: [[[-0.2815268   0.36459672  0.45034242 ... -0.47318697 -0.40278506\n",
      "   -0.24017537]\n",
      "  [ 0.16104491 -0.49883014 -0.05703727 ... -0.6232359  -0.23104468\n",
      "   -0.20491666]\n",
      "  [-1.1852155  -1.002929   -0.608784   ... -0.5141611  -0.37379324\n",
      "   -0.2654637 ]\n",
      "  ...\n",
      "  [ 0.01822925 -0.3753832  -0.8963473  ...  0.17242727  0.05693132\n",
      "    0.1306364 ]\n",
      "  [ 0.26943898 -0.06628752 -0.5013158  ... -1.4431518  -1.4824082\n",
      "   -1.4502244 ]\n",
      "  [-1.5649999  -1.669118   -1.2826786  ...  0.511411    0.48322338\n",
      "    0.01353467]]]\n",
      "\n",
      "Transcription: Ничьих не требуя похвал, Счастлив уж я надеждой сладкой, Что дева с трепетом любви Посмотрит, может быть, украдкой На песни грешные мои. У лукоморья дуб зелёный.\n",
      "\n",
      "Emotions: [[7.7093333e-05 2.2028047e-03 9.2327267e-01 7.4447356e-02]]\n"
     ]
    }
   ],
   "source": [
    "from gigaam.onnx_utils import load_onnx, infer_onnx\n",
    "\n",
    "sessions, model_cfg = load_onnx(onnx_dir, \"v2_ssl\")\n",
    "print(\"Embeds:\", infer_onnx(audio_path, model_cfg, sessions))\n",
    "\n",
    "sessions, model_cfg = load_onnx(onnx_dir, \"v3_e2e_rnnt\")\n",
    "print(\"\\nTranscription:\", infer_onnx(audio_path, model_cfg, sessions))\n",
    "\n",
    "sessions, model_cfg = load_onnx(onnx_dir, \"emo\")\n",
    "print(\"\\nEmotions:\", infer_onnx(audio_path, model_cfg, sessions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf16N9yaxGo"
   },
   "source": [
    "### Longform\n",
    "\n",
    "As `.transcribe` function input lenght is limited by 25 seconds, we need `.transcribe_longform` with audio segmentation based on `pyannote/segmentation-3.0` voice activity detection pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiX5t2CHb1QB"
   },
   "outputs": [],
   "source": [
    "! pip install -e .[longform]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jq3NiXRR1nts",
    "outputId": "76b5606a-56a8-47e7-9ae8-d9c911313ed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gigaam\n"
     ]
    }
   ],
   "source": [
    "# in colab the session might need restarting here\n",
    "%cd GigaAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DB8qh_X1bUL_",
    "outputId": "387aea4b-382f-4348-9eae-8b51da2c7026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /content/gigaam\n",
      "plugins: cov-7.0.0, hydra-core-1.3.2, anyio-4.11.0, typeguard-4.4.4, langsmith-0.4.42\n",
      "collected 7 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_longform.py::test_segmentation_functionality[30.0] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 14%]\u001b[0m\n",
      "tests/test_longform.py::test_segmentation_functionality[60.0] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 28%]\u001b[0m\n",
      "tests/test_longform.py::test_segmentation_functionality[120.0] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 42%]\u001b[0m\n",
      "tests/test_longform.py::test_transcribe_longform[v3_ctc] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 57%]\u001b[0m\n",
      "tests/test_longform.py::test_transcribe_longform[v3_e2e_rnnt] \u001b[32mPASSED\u001b[0m\u001b[33m     [ 71%]\u001b[0m\n",
      "tests/test_longform.py::test_longform_consistency[v3_ctc] \u001b[32mPASSED\u001b[0m\u001b[33m         [ 85%]\u001b[0m\n",
      "tests/test_longform.py::test_segmentation_edge_cases \u001b[32mPASSED\u001b[0m\u001b[33m              [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m======================== \u001b[32m7 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 56.39s\u001b[0m\u001b[33m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! HF_TOKEN=\"<your hf token>\" pytest -v tests/test_longform.py --disable-warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFlpV4VXapk7",
    "outputId": "17a36be8-d1db-4ce8-cf33-5812b93c5f14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00:00 - 00:16:80]: Вечерня отошла давно, Но в кельях тихо и темно; Уже и сам игумен строгий Свои молитвы прекратил И кости ветхие склонил, Перекрестясь на одр убогий. Кругом и сон, и тишина; Но церкви дверь отворена.\n",
      "[00:17:07 - 00:32:54]: Трепещет луч лампады, И тускло озаряет он И тёмную живопись икон, и возглащённые оклады. И раздаётся в тишине То тяжкий вздох, то шёпот важный, И мрачно дремлет в тишине старинный свод.\n",
      "[00:32:95 - 00:49:30]: Глухой и влажный Стоят за клиросом чернец и грешник, Неподвижны оба. И шёпот их — Как глаз из гроба, И грешник бледен, как мертвец — Монах. Несчастный! Полно, перестань!\n",
      "[00:49:81 - 01:05:65]: Ужасна исповедь злодея, Заплачена тобою дань Тому, Кто в злобе пламенея Лукавого грешника блюдёт И к вечной гибели ведёт. Смирись, опомнись. Время, время. Раскаянье, покров\n",
      "[01:05:94 - 01:10:88]: Я разрешу тебя, грехов сложи мучительное бремя.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import gigaam\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"<HF_TOKEN with read access to `pyannote/segmentation-3.0`>\"\n",
    "\n",
    "long_audio_path = gigaam.utils.download_long_audio()\n",
    "model = gigaam.load_model(\"v3_e2e_rnnt\")\n",
    "\n",
    "utterances = model.transcribe_longform(long_audio_path)\n",
    "for utt in utterances:\n",
    "   transcription, (start, end) = utt[\"transcription\"], utt[\"boundaries\"]\n",
    "   print(f\"[{gigaam.format_time(start)} - {gigaam.format_time(end)}]: {transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBwAxFger9vB"
   },
   "source": [
    "### More advanced examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNPBdICGsqio"
   },
   "source": [
    "##### Another way to load audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fmwFIE9qaf7y",
    "outputId": "9d85b25e-0346-4306-cf3c-131a6a63af1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ничьих не требуя похвал, Счастлив уж я надеждой сладкой, Что дева с трепетом любви Посмотрит, может быть, украдкой На песни грешные мои. У лукоморья дуб зелёный.', 'Ничьих не требуя похвал, Счастлив уж я надеждой сладкой, Что дева с трепетом любви Посмотрит, может быть, украдкой На песни грешные мои. У лукоморья дуб зелёный.']\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import torch\n",
    "\n",
    "from gigaam.utils import AudioDataset\n",
    "from gigaam.preprocess import SAMPLE_RATE as SR\n",
    "\n",
    "fname = gigaam.utils.download_short_audio()\n",
    "wav = gigaam.load_audio(fname)\n",
    "# arrays can be not equal after possible resampling, but close enough\n",
    "wav_ = torch.from_numpy(librosa.load(fname, sr=SR, mono=True)[0])\n",
    "\n",
    "wav_tns, lengths = AudioDataset.collate([wav, wav_])\n",
    "with torch.no_grad():\n",
    "    encoded, encoded_len = model(\n",
    "        wav_tns.to(model._device).to(model._dtype), lengths.to(model._device)\n",
    "    )\n",
    "    print(model.decoding.decode(model.head, encoded, encoded_len))\n",
    "\n",
    "# outputs expected to be equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIeaDNXYsw44"
   },
   "source": [
    "##### Longform & batch > 1\n",
    "\n",
    "We'll demonstrate batching with an example for longform inference. Here a long audio is split into independent segments - making it ideal for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMqA7Kbosu1k"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "import gigaam\n",
    "from gigaam.preprocess import SAMPLE_RATE as SR\n",
    "from gigaam.vad_utils import segment_audio_file\n",
    "\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"<HF_TOKEN with read access to `pyannote/segmentation-3.0`>\"\n",
    "\n",
    "exp_dir = Path(\"tmp_inference_example\")\n",
    "exp_dir.mkdir(exist_ok=True)\n",
    "long_fname = gigaam.utils.download_long_audio()\n",
    "model = gigaam.load_model(\"v3_e2e_rnnt\")\n",
    "\n",
    "# Saving parts as files\n",
    "with torch.inference_mode():\n",
    "    segments, boundaries = segment_audio_file(long_fname, sr=SR, device=model._device)\n",
    "wav_paths = []\n",
    "for i, segment in enumerate(segments):\n",
    "    wav_paths.append(str(exp_dir / f\"{Path(long_fname).stem}_{i}{Path(long_fname).suffix}\"))\n",
    "    sf.write(wav_paths[-1], segment, SR)\n",
    "\n",
    "# Load wavs: you can use their paths or load with librosa-like method as above\n",
    "wavs_np = [librosa.load(wav_path, sr=SR, mono=True)[0] for wav_path in wav_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6OcwpVqt05n"
   },
   "source": [
    "Finally, run batched inference. In our case the input audio is fairly short, `batch_size` can be decreased for longer inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGeCADORttRY",
    "outputId": "2cff3399-663c-497a-d41a-2d028a3de65c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00:00 - 00:16:80]: Вечерня отошла давно, Но в кельях тихо и темно; Уже и сам игумен строгий Свои молитвы прекратил И кости ветхие склонил, Перекрестясь на одр убогий. Кругом и сон, и тишина; Но церкви дверь отворена.\n",
      "[00:17:07 - 00:32:54]: Трепещет луч лампады, И тускло озаряет он И тёмную живопись икон, и возглащённые оклады. И раздаётся в тишине То тяжкий вздох, то шёпот важный, И мрачно дремлет в тишине старинный свод.\n",
      "[00:32:95 - 00:49:30]: Глухой и влажный Стоят за клиросом чернец и грешник, Неподвижны оба. И шёпот их — Как глаз из гроба, И грешник бледен, как мертвец — Монах. Несчастный! Полно, перестань!\n",
      "[00:49:81 - 01:05:65]: Ужасна исповедь злодея, Заплачена тобою дань Тому, Кто в злобе пламенея Лукавого грешника блюдёт И к вечной гибели ведёт. Смирись, опомнись. Время, время. Раскаянье, покров\n",
      "[01:05:94 - 01:10:88]: Я разрешу тебя, грехов сложи мучительное бремя.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dataset = AudioDataset(wavs_np)  # or AudioDataset(wav_paths)\n",
    "batch_size = len(dataset)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, collate_fn=dataset.collate, shuffle=False\n",
    ")\n",
    "\n",
    "pred_texts = []\n",
    "for wav_tns, lengths in dataloader:\n",
    "    wav_tns, lengths = wav_tns.to(model._device).to(model._dtype), lengths.to(model._device)\n",
    "    with torch.no_grad():\n",
    "        encoded, encoded_len = model(wav_tns, lengths)\n",
    "    pred_texts.extend(model.decoding.decode(model.head, encoded, encoded_len))\n",
    "\n",
    "for (start, end), text in zip(boundaries, pred_texts):\n",
    "    print(f\"[{gigaam.format_time(start)} - {gigaam.format_time(end)}]: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ipQYBdYcQH0"
   },
   "source": [
    "## GigaAM from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FJqALokc5vx"
   },
   "source": [
    "### Reqs and files loading: now by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EAqie1xco_I"
   },
   "outputs": [],
   "source": [
    "! pip install numpy==2.* torch==2.8.* torchaudio==2.8.* transformers==4.57.* \\\n",
    "    pyannote.audio==4.0 torchcodec==0.7 numba>=0.62 \\\n",
    "    onnx==1.19.* onnxruntime==1.23.* \\\n",
    "    hydra-core==1.3.* omegaconf==2.3.* \\\n",
    "    sentencepiece tqdm\n",
    "! pip install --force-reinstall numpy  # make transformers work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nMLdr0VvIGj"
   },
   "outputs": [],
   "source": [
    "! wget https://cdn.chatwm.opensmodel.sberdevices.ru/GigaAM/example.wav\n",
    "! wget https://cdn.chatwm.opensmodel.sberdevices.ru/GigaAM/long_example.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267,
     "referenced_widgets": [
      "2e3f6159ef5e407e8f1fb9bca32162b7",
      "4b76d95921354dc785fe6193f2925317",
      "c23e0e9417cc44a7bb653a7a56b954d0",
      "ea66c0bd776e4845834237d8c5d4837b",
      "5441e4df284e49408cb8be1271039aa4",
      "ac4f18edb19b468f963e771fdb1dc12e",
      "6f3018569f1a400fb15e4eda9eb5ec8e",
      "bd2462f8e23f4bb1ba8d50bb73c64699",
      "fff6474730784e089f6d956b8e2bd1a0",
      "d61a763ea9884978aef9261c818f3c83",
      "3c95709c357f4869812159549a0edffb",
      "ea8dd50349f342c6b0b53cfab3e4437a",
      "d7820d5e55964d4cac1989638bb70a6d",
      "e30d397d613b48e9916d5f2a0e3acd8e",
      "164df35c2a65427e800b6a17fdb90438",
      "593ccb48450e43ae9df220123bfc2c85",
      "69c0084f5032462687d770528342c803",
      "c046db1ead5c4b309d7e3e127661ccc4",
      "6bbccc048f264a7a91bdcc520d25e492",
      "1b1adb6e50da4cb7a94a01d1315331f6",
      "83fb6d82fab347708f7dd10b98340a1a",
      "3fe5ac311b244176b2d61c49c20986e7",
      "604c907ea10344c996e6d5d09896d7bf",
      "31aaa1dc03e042a8820692d86b198eb3",
      "920b77ca466643528c108e56cdd65aa5",
      "6ce4197e475b44ca8ddd47f1c9fc4393",
      "693b1d83cb7044afa437ce811f5b864a",
      "62178bcb8395445baf7ec87d43ff1956",
      "5034511b0e9c41adb9de422f60be5db9",
      "23272696a2b34ac4883abb80b2f80be1",
      "65d81f90e2014bdc8af2348a16a22919",
      "38fa41796aea4b7ebe2f552a76691f30",
      "43b54378f65347bb97b8a9958fe0e141",
      "c4c239fbddbd49428c53e99e4c6d7395",
      "94b3977244d84269a9b12ee836d1eb81",
      "040db1bc88cc4798a257d30d61e22f0d",
      "3fde9901a2254aa79c42c9b510e2f293",
      "8b04a773cc3a46508992d4145864f9bc",
      "ed0e21bd550b4fd38d47a55377eedb6c",
      "909ae2153eb04d3394bad1298787b56e",
      "cf99eeeb1b394df4af5adaa674aabffc",
      "29909a4fc3fa48c2b34f59840f7889c3",
      "510a7cbe840140118b8c850545a11aab",
      "f4443110254141e8a1366a3d7a54ef93"
     ]
    },
    "id": "UCY_7Ltya2Wt",
    "outputId": "a5924f29-ea27-4181-aa56-dbef579875f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3f6159ef5e407e8f1fb9bca32162b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8dd50349f342c6b0b53cfab3e4437a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_gigaam.py:   0%|          | 0.00/49.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604c907ea10344c996e6d5d09896d7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/449M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c239fbddbd49428c53e99e4c6d7395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/255k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"ai-sage/GigaAM-v3\", revision=\"e2e_rnnt\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-whdLDSdWmf"
   },
   "source": [
    "### Everything else is the same!\n",
    "\n",
    "> Note: `fp16` encoder on CUDA can be used with `model.model.encoder = model.model.encoder.half()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OlYBRz0dVcA",
    "outputId": "b5cfac2c-0d94-452f-acbb-c69cbd0155ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed: tensor([[[-0.2815,  0.3646,  0.4503,  ..., -0.4732, -0.4028, -0.2402],\n",
      "         [ 0.1610, -0.4988, -0.0570,  ..., -0.6232, -0.2310, -0.2049],\n",
      "         [-1.1852, -1.0029, -0.6088,  ..., -0.5142, -0.3738, -0.2655],\n",
      "         ...,\n",
      "         [ 0.0182, -0.3754, -0.8963,  ...,  0.1724,  0.0569,  0.1306],\n",
      "         [ 0.2694, -0.0663, -0.5013,  ..., -1.4432, -1.4824, -1.4502],\n",
      "         [-1.5650, -1.6691, -1.2827,  ...,  0.5114,  0.4832,  0.0135]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Transcription: Ничьих не требуя похвал, Счастлив уж я надеждой сладкой, Что дева с трепетом любви Посмотрит, может быть, украдкой На песни грешные мои. У лукоморья дуб зелёный.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "repo_name = \"ai-sage/GigaAM-v3\"\n",
    "\n",
    "# Audio embeddings\n",
    "model_name = \"ssl\"\n",
    "model = AutoModel.from_pretrained(repo_name, revision=model_name, trust_remote_code=True).to(device)\n",
    "embedding, _ = model.embed_audio(\"example.wav\")\n",
    "print(\"Embed:\", embedding)\n",
    "\n",
    "# ASR\n",
    "model_name = \"e2e_rnnt\"  # Options: rnnt, ctc, e2e_rnnt, e2e_ctc\n",
    "model = AutoModel.from_pretrained(repo_name, revision=model_name, trust_remote_code=True).to(device)\n",
    "transcription = model.transcribe(\"example.wav\")\n",
    "print(\"\\nTranscription:\", transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ek1gKS6_cW-G",
    "outputId": "39308dfb-f63d-4eb5-d944-30bcf6d7e462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longform transcription:\n",
      "\n",
      "[0.0000 - 16.8047]: Вечерня отошла давно, Но в кельях тихо и темно; Уже и сам игумен строгий Свои молитвы прекратил И кости ветхие склонил, Перекрестясь на одр убогий. Кругом и сон, и тишина; Но церкви дверь отворена.\n",
      "[17.0747 - 32.5491]: Трепещет луч лампады, И тускло озаряет он И тёмную живопись икон, и возглащённые оклады. И раздаётся в тишине То тяжкий вздох, то шёпот важный, И мрачно дремлет в тишине старинный свод.\n",
      "[32.9541 - 49.3060]: Глухой и влажный Стоят за клиросом чернец и грешник, Неподвижны оба. И шёпот их — Как глаз из гроба, И грешник бледен, как мертвец — Монах. Несчастный! Полно, перестань!\n",
      "[49.8122 - 65.6578]: Ужасна исповедь злодея, Заплачена тобою дань Тому, Кто в злобе пламенея Лукавого грешника блюдёт И к вечной гибели ведёт. Смирись, опомнись. Время, время. Раскаянье, покров\n",
      "[65.9447 - 70.8891]: Я разрешу тебя, грехов сложи мучительное бремя.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"<HF_TOKEN with read access to `pyannote/segmentation-3.0`>\"\n",
    "\n",
    "model = AutoModel.from_pretrained(repo_name, revision=\"e2e_rnnt\", trust_remote_code=True)\n",
    "utterances = model.transcribe_longform(\"long_example.wav\")\n",
    "print(\"Longform transcription:\\n\")\n",
    "for utt in utterances:\n",
    "   transcription, (start, end) = utt[\"transcription\"], utt[\"boundaries\"]\n",
    "   print(f\"[{start:.4f} - {end:.4f}]: {transcription}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBG1U5TOrB24",
    "outputId": "5f8e0707-a6ed-4e4f-fc45-78c31465e8fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully ported onnx v3_e2e_rnnt_encoder to hf_onnx/v3_e2e_rnnt_encoder.onnx.\n",
      "Succesfully ported onnx v3_e2e_rnnt_decoder to hf_onnx/v3_e2e_rnnt_decoder.onnx.\n",
      "Succesfully ported onnx v3_e2e_rnnt_joint to hf_onnx/v3_e2e_rnnt_joint.onnx.\n"
     ]
    }
   ],
   "source": [
    "model.to_onnx(\"hf_onnx\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
